{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.11", "language": "python"}, "language_info": {"name": "python", "version": "3.11.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "### Watson creds & Model parameters", "metadata": {"id": "70e3d6c2-8919-4581-84c8-15ea11d40635"}}, {"cell_type": "code", "source": "pip install ibm-watson", "metadata": {"id": "ab0c888d-a5ab-415c-924a-dbcbaf1f48b8", "msg_id": "b125265c-20b3-4eb2-938e-12f6271963fd"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "import os\nfrom ibm_watsonx_ai import APIClient, Credentials\nimport getpass\n\ncredentials = Credentials(\n    url=\"https://us-south.ml.cloud.ibm.com\",\n    api_key= \"bCXPf2hORlhnSHfSKjn_2ZPKKu3TyeJldu0vz0445gL2\"\n)", "metadata": {"id": "7d0b072f-f693-4c48-9a6f-b6b7a2887fe7", "msg_id": "b2ed7051-4e71-45d5-beab-987acfe4e86f"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "model_id = \"meta-llama/llama-3-405b-instruct\"", "metadata": {"id": "da369657-83d6-4717-8927-3b2932e9c7b8", "msg_id": "7b3b6a70-1c93-4dc3-97a6-c61e512aa0cd"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "parameters = {\n    \"frequency_penalty\": 0,\n    \"max_tokens\": 2000,\n    \"presence_penalty\": 0,\n    \"temperature\": 0,\n    \"top_p\": 1\n}", "metadata": {"id": "ed2cb310-bb1a-4b2f-91d5-9ce9c07ab90d", "msg_id": "b6e82fcf-26d8-4b7e-87e6-557c49c77120"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "project_id = os.getenv(\"PROJECT_ID\")\nspace_id = os.getenv(\"SPACE_ID\")", "metadata": {"id": "b8ffb12d-eee1-4bb1-85ae-81dd875b34bf", "msg_id": "2bdd6cb4-1048-4be2-bddb-f242b205a72c"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "from ibm_watsonx_ai.foundation_models import ModelInference\n\nmodel = ModelInference(\n\tmodel_id = model_id,\n\tparams = parameters,\n\tcredentials = credentials,\n\tproject_id = project_id,\n\tspace_id = space_id\n\t)", "metadata": {"id": "6600759c-5e0c-4ff3-9a19-f58347a5be90", "msg_id": "322d29ea-9f8b-4a4b-b975-278aa674293e"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### Web page fetching & content cleanup function (extract text from html page)", "metadata": {"id": "9fa72c6b-fa51-435a-bf7a-41f286365f13"}}, {"cell_type": "code", "source": "import requests\nfrom bs4 import BeautifulSoup\nimport re # Import the regular expression module\n\ndef extract_clean_text_from_url(url):\n    \"\"\"\n    Extracts clean, narrative text from a given URL, specifically optimized for Wikipedia pages.\n    Removes common boilerplate, navigation, and citation noise.\n\n    Args:\n        url (str): The URL of the webpage to extract text from.\n\n    Returns:\n        str: The cleaned text content of the webpage, or an error message.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        # --- Step 1: Identify the main content area ---\n        # For Wikipedia, the primary article content is usually within 'mw-content-text' div.\n        # Fallback to 'mw-parser-output' if the more specific ID isn't found.\n        # If neither is found, we process the whole soup, but this is less ideal for Wikipedia.\n        main_content_div = soup.find(id='mw-content-text')\n        if not main_content_div:\n            main_content_div = soup.find(class_='mw-parser-output')\n            if not main_content_div:\n                main_content_div = soup # Fallback to entire soup if main content div is not clearly identifiable\n\n        # --- Step 2: Aggressively remove unwanted tags and elements ---\n        # These are common noise elements found on many websites, especially Wikipedia.\n        unwanted_tags = ['script', 'style', 'noscript', 'header', 'footer', 'nav', 'aside',\n                         'form', 'img', 'link', 'meta', 'input', 'button', 'select',\n                         'textarea', 'label', 'iframe', 'svg', 'canvas', 'audio', 'video']\n\n        for tag_name in unwanted_tags:\n            for tag in main_content_div.find_all(tag_name):\n                tag.decompose()\n\n        # Remove common Wikipedia-specific noise elements by ID or Class\n        # Applying these directly to 'soup' first to ensure global removal of these structural elements.\n        global_unwanted_ids = ['siteNotice', 'mw-navigation', 'mw-panel', 'footer',\n                               'catlinks', 'coordinates', 'mw-js-message', 'toc']\n        for unwanted_id in global_unwanted_ids:\n            if soup.find(id=unwanted_id):\n                soup.find(id=unwanted_id).decompose()\n\n        global_unwanted_classes = ['printfooter', 'mw-indicators', 'portal', 'sister-project',\n                                   'navbox', 'vertical-navbox', 'metadata', 'ambox',\n                                   'box-Multiple_issues', 'hatnote', 'dablink', 'image', 'gallery',\n                                   'vector-body-before-content']\n        for unwanted_class in global_unwanted_classes:\n            for tag in soup.find_all(class_=unwanted_class):\n                tag.decompose()\n\n        # Specific Wikipedia content types that are often non-narrative but within main content:\n        # Tables (e.g., infoboxes, lists), reference sections, and \"edit\" links.\n        for tag in main_content_div.find_all(['table', 'ul', 'ol', 'dl', 'sup']): # sup for citations\n            tag.decompose()\n\n        # --- Step 3: Extract text and perform post-processing ---\n        # Get all text strings, join them, and then clean up.\n        text = ' '.join(main_content_div.stripped_strings)\n\n        # Remove common \"[ edit ]\" markers after headings\n        text = text.replace('[ edit ]', '').strip()\n\n        # Remove numbers/letters in square brackets (citations like [1], [a], etc.)\n        text = re.sub(r'\\[\\d+\\]', '', text)       # Removes [1], [23], etc.\n        text = re.sub(r'\\[[a-zA-Z]\\]', '', text)  # Removes [a], [b], etc.\n        text = re.sub(r'\\[\\s*\\d+\\s*\\]', '', text) # Handle potential spaces like [ 1 ]\n\n        # Normalize whitespace: replace multiple spaces/newlines/tabs with a single space\n        text = ' '.join(text.split())\n\n        return text\n\n    except requests.exceptions.RequestException as e:\n        return f\"Network or HTTP error: {e}\"\n    except Exception as e:\n        return f\"An unexpected error occurred: {e}\"\n\n# --- Usage Example ---\nurl = \"https://en.wikipedia.org/wiki/Abdullah_II_of_Jordan\"\nextracted_text = extract_clean_text_from_url(url)", "metadata": {"id": "78121a0b-3ad7-46cd-bd16-ad5d66743dbd", "msg_id": "69c88742-866b-4cba-8418-de7d5ec2adc6"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### Watson Discovery Creds & Client", "metadata": {"id": "95f69866-970f-47d0-a16b-4f046a3e1af7"}}, {"cell_type": "code", "source": "DISCOVERY_API_KEY = \"CXN0kFFvK3W_F6yK5bb6P3W-YAd56nglC_SFN5e8_76d\"\nDISCOVERY_SERVICE_URL = \"https://api.eu-gb.discovery.watson.cloud.ibm.com/instances/e197186b-f3ab-42d8-9472-1f87fbcfdea6\"\nDISCOVERY_PROJECT_ID = \"b24408fe-3689-4a29-9aa0-f5a43d3f3799\"\nDISCOVERY_COLLECTION_ID = \"76c98ff5-ced8-3a0e-0000-019826aef743\" \n\n# The version of the Discovery API to use (recommended to use a recent stable version)\n# Check IBM Watson Discovery documentation for the latest recommended version.\nDISCOVERY_API_VERSION = \"2023-03-31\"\n\nprint(\"Watson Discovery configuration variables set.\")", "metadata": {"id": "374dd80c-0c5c-4103-b004-9fe52ea2b4d8", "msg_id": "4a45b85f-b078-4c21-b1d4-81213d43b64a"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "from ibm_watson import DiscoveryV2\nfrom ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n\ntry:\n    # Authenticate with IAM\n    authenticator = IAMAuthenticator(DISCOVERY_API_KEY)\n\n    # Initialize the Discovery client\n    discovery_client = DiscoveryV2(\n        version=DISCOVERY_API_VERSION,\n        authenticator=authenticator\n    )\n\n    # Set the service URL\n    discovery_client.set_service_url(DISCOVERY_SERVICE_URL)\n\n    print(\"Watson Discovery client initialized successfully.\")\nexcept Exception as e:\n    print(f\"ERROR: Failed to initialize Watson Discovery client: {e}\")\n    print(\"Please check your API Key, Service URL, and ensure the SDK is installed correctly.\")", "metadata": {"id": "d7173f78-5ec2-4ef5-b120-24161c6fa000", "msg_id": "9c02a2b9-ee68-4832-bb28-8f31a88d5f39"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### Upload content to Watson Discovery for processing", "metadata": {"id": "2ae2eb32-d929-4170-b79e-bb19c063e79c"}}, {"cell_type": "code", "source": "import io\nimport datetime\n\ndef upload_text_to_discovery(text):\n    try:\n        # Create an in-memory text file from the extracted text\n        file_obj = io.BytesIO(text.encode(\"utf-8\"))\n\n        # Create a unique name using timestamp or hash if needed\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        doc_name = f\"doc_{timestamp}.txt\"\n\n        # Upload the document to Discovery\n        response = discovery_client.add_document(\n            project_id=DISCOVERY_PROJECT_ID,\n            collection_id=DISCOVERY_COLLECTION_ID,\n            file=file_obj,\n            filename=doc_name,\n            file_content_type=\"text/plain\"\n        ).get_result()\n\n        document_id = response.get(\"document_id\")\n        print(f\"Document uploaded successfully. Document ID: {document_id}\")\n        return document_id\n\n    except Exception as e:\n        print(f\"Failed to upload document to Discovery: {e}\")\n        return None\n\n\n# --- Upload the Wikipedia extracted text ---\ndocument_id = upload_text_to_discovery(extracted_text)\n\n", "metadata": {"id": "ebe7f424-6fac-4a24-900e-e7953eac6ab0", "msg_id": "dc37110f-9e14-400e-a8bf-dd9a7503cd32"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "import time\n\ndef check_status_until_available(document_id, discovery_client, project_id, collection_id,\n                                 interval_seconds=10, timeout_minutes=10, _start_time=None):\n    \"\"\"\n    Recursively checks the document's processing status until it's 'available',\n    or a timeout is reached. Prints status at each check.\n    \"\"\"\n    if _start_time is None:\n        _start_time = time.time()\n    \n    elapsed = time.time() - _start_time\n\n    # Base case: timeout reached\n    if elapsed > timeout_minutes * 60:\n        print(f\"\u23f0 TIMEOUT: Document ID {document_id} still not available after {timeout_minutes} minutes.\")\n        return False\n\n    try:\n        metadata = discovery_client.get_document(\n            project_id=project_id,\n            collection_id=collection_id,\n            document_id=document_id\n        ).get_result()\n\n        status = metadata.get(\"status\", \"unknown\")\n        print(f\"\ud83d\udce1 Status Check: {status} | Elapsed: {int(elapsed)}s\")\n\n        if status == \"available\":\n            print(f\"\u2705 Document is ready for querying (status: available).\")\n            return True\n        elif status == \"failed\":\n            print(f\"\u274c Document processing failed.\")\n            return False\n        else:\n            time.sleep(interval_seconds)\n            return check_status_until_available(\n                document_id, discovery_client, project_id, collection_id,\n                interval_seconds, timeout_minutes, _start_time\n            )\n\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Error while checking document status: {e}\")\n        time.sleep(interval_seconds * 2)\n        return check_status_until_available(\n            document_id, discovery_client, project_id, collection_id,\n            interval_seconds, timeout_minutes, _start_time\n        )\n\n# After uploading the document\nstatus_ready = check_status_until_available(\n    document_id=document_id,\n    discovery_client=discovery_client,\n    project_id=DISCOVERY_PROJECT_ID,\n    collection_id=DISCOVERY_COLLECTION_ID\n)\n\nif status_ready:\n    print(\"\u2705 You may now safely proceed to query the document.\")\nelse:\n    print(\"\ud83d\udeab Aborting: Document is not available.\")\n    raise \"Document is not available\"\n\n", "metadata": {"id": "8315f018-e1ea-42cc-9df4-b0302a1c640e", "msg_id": "e0560f39-7469-482c-9ea2-8c6b1674a1a2"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "import json\n\ntry:\n    # Perform a query on the Discovery collection to retrieve enriched entities and FULL TEXT from the document\n    response = discovery_client.query(\n        project_id=DISCOVERY_PROJECT_ID,\n        collection_ids=[DISCOVERY_COLLECTION_ID],\n        filter=f'document_id::\"{document_id}\"',\n        # Now explicitly asking for the 'text' field to get the full content\n        return_=['text', 'enriched_text.entities.text', 'enriched_text.entities.type','enriched_text.entities.model']\n    ).get_result()\n\n    results = response.get(\"results\", [])\n    first_document_result = results[0]\n\nexcept Exception as e:\n    print(f\"ERROR: Failed to query content from the document: {e}\")", "metadata": {"id": "d0222944-b68f-4375-bc00-3995dffceffc", "msg_id": "639812f0-6967-4df1-abc5-6fb8fa0d3ebe"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# This cell assumes first_document_result is already defined from the previous query cell\n\nenriched_text_content_list = first_document_result.get(\"enriched_text\", [])\n\nif enriched_text_content_list and isinstance(enriched_text_content_list, list) and enriched_text_content_list:\n    # Access the first dictionary in the list which contains the 'entities'\n    # Adding a check for the content of the list\n    if isinstance(enriched_text_content_list[0], dict):\n        all_entities = enriched_text_content_list[0].get(\"entities\", [])\n\n        # Filter the entities to only keep those from \"extractor1\" model\n        extractor1_entities = [\n            entity for entity in all_entities\n            if entity.get('model_name') == 'extractor'\n        ]\n\n        if extractor_entities:\n            print(\"\\nExtracted Entities (filtered for 'extractor' model):\")\n            for entity in extractor1_entities: # Iterate through the FILTERED list\n                entity_text = entity.get('text', 'N/A')\n                entity_type = entity.get('type', 'N/A')\n                print(f\"- {entity_text} (Type: {entity_type})\")\n        else:\n            print(\"No entities from 'extractor' model found in the 'enriched_text' section of the document.\")\n    else:\n        print(\"The first element of 'enriched_text' was not a dictionary as expected.\")\nelse:\n    print(\"The 'enriched_text' field was not in the expected list format or was empty.\")", "metadata": {"id": "3baba6cf-b729-4c4e-99b4-8f4c1a9ca8a6", "msg_id": "6863e009-7ada-495a-a3c4-4bdb174e3e0a"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "import json\nfrom ibm_watson import NaturalLanguageUnderstandingV1\nfrom ibm_cloud_sdk_core.authenticators import IAMAuthenticator\nfrom ibm_watson.natural_language_understanding_v1 import Features, RelationsOptions\n\n# --- Configuration ---\n# Replace with your IBM Cloud API Key for Natural Language Understanding\n# You can find this in your IBM Cloud service credentials.\nAPI_KEY = \"kDJtnjEIdClO2D--_TeYQQuSxi1bIMprtXelLe-QOfZs\"\n\n# Replace with your NLU service URL (e.g., 'https://api.us-south.natural-language-understanding.watson.cloud.ibm.com/instances/...')\n# You can find this in your IBM Cloud service credentials.\nSERVICE_URL = \"https://api.au-syd.natural-language-understanding.watson.cloud.ibm.com/instances/25c92d43-10fc-4d50-b92b-cebec923e67c\"\n\n# The text you want to analyze for relations\ntext_to_analyze = extracted_text\n\n# --- Initialize NLU Service ---\ntry:\n    authenticator = IAMAuthenticator(API_KEY)\n    natural_language_understanding = NaturalLanguageUnderstandingV1(\n        version='2022-04-07', # Use a recent API version\n        authenticator=authenticator\n    )\n    natural_language_understanding.set_service_url(SERVICE_URL)\n\n    print(\"Watson Natural Language Understanding service initialized successfully.\\n\")\n\n    # --- Define Features for Analysis ---\n    # We are specifically interested in 'relations'\n    features = Features(\n        relations=RelationsOptions()\n    )\n\n    # --- Analyze the Text ---\n    print(\"Analyzing text for relations...\\n\")\n    response = natural_language_understanding.analyze(\n        text=text_to_analyze,\n        features=features\n    ).get_result()\n\n    # --- Process and Print Results ---\n    if 'relations' in response and len(response['relations']) > 0:\n        print(\"Extracted Relations:\")\n        for relation in response['relations']:\n            print(f\"  Type: {relation.get('type')}\")\n            print(f\"  Score: {relation.get('score'):.2f}\")\n            print(f\"  Sentence: {relation.get('sentence')}\")\n            print(f\"  Arguments:\")\n            for arg in relation.get('arguments', []):\n                print(f\"    - Text: {arg.get('text')}\")\n                print(f\"      Type: {arg.get('entities', [{}])[0].get('type')}\") # Get type from first entity if available\n            print(\"-\" * 30)\n    else:\n        print(\"No relations found in the provided text.\")\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n    print(\"Please ensure your API_KEY and SERVICE_URL are correct and you have access to the NLU service.\")\n\n", "metadata": {"id": "ab6553e0-ad6d-4d2a-b791-bb6ced93ad82", "msg_id": "6fa38160-1305-4177-94b8-c72c94c7f5eb"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "!pip install neo4j", "metadata": {"id": "9acb790c-e5a0-48d2-873a-637d4e3f30d8", "msg_id": "bd8c1ff0-5270-4b13-bb43-b457f27a7cfb"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "from neo4j import GraphDatabase\n\nNEO4J_URI = \"neo4j+s://5280f5c3.databases.neo4j.io\"\nNEO4J_USER = \"neo4j\"\nNEO4J_PASSWORD = \"fY0fNCGlWaBhR73KVDjlQVVvKxvm3Q5n70uLJiMYveg\"\n\ndriver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n\ntry:\n    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n    driver.verify_connectivity()\n    print(\"Successfully connected to Neo4j AuraDB!\")\nexcept Exception as e:\n    print(f\"ERROR: Could not connect to Neo4j: {e}\")\n    driver = None ", "metadata": {"id": "2c19f370-d2a9-47e4-b83d-06dc43b61305", "msg_id": "927fd4cb-3cc4-4a6a-8bb4-8fc7a45a530e"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "import re\n\ndef sanitize_relationship(rel):\n    \"\"\"\n    Converts a relationship string into a valid Cypher relationship type.\n    - Replaces spaces and invalid characters with underscores.\n    - Ensures it only contains alphanumeric characters and underscores.\n    \"\"\"\n    rel = rel.strip()\n    rel = rel.replace(\" \", \"_\")  # Replace spaces with underscores\n    rel = re.sub(r\"[^A-Za-z0-9_]\", \"_\", rel)  # Replace invalid characters with underscores\n    rel = re.sub(r\"_+\", \"_\", rel)  # Replace multiple underscores with a single one\n    rel = rel.strip(\"_\")  # Remove leading/trailing underscores if any\n    if not rel:\n        rel = \"RELATED_TO\"  # Fallback if relation becomes empty\n    return rel\n\ndef save_relationships_to_neo4j(llm_output, driver):\n    \"\"\"\n    Parses relationships from LLM output and saves them to a Neo4j graph.\n    \"\"\"\n    pattern = r\"\\{(.*?),\\s*(.*?),\\s*(.*?)\\}\"\n    matches = re.findall(pattern, llm_output)\n\n    with driver.session() as session:\n        for entity1, relation, entity2 in matches:\n            entity1 = entity1.strip()\n            entity2 = entity2.strip()\n            sanitized_relation = sanitize_relationship(relation)\n\n            print(f\"Inserting: ({entity1}) -[:{sanitized_relation}]-> ({entity2})\")\n\n            cypher_query = f\"\"\"\n            MERGE (a:Entity {{name: $entity1}})\n            MERGE (b:Entity {{name: $entity2}})\n            MERGE (a)-[r:{sanitized_relation}]->(b)\n            \"\"\"\n\n            session.run(cypher_query, entity1=entity1, entity2=entity2)\n", "metadata": {"id": "c81a431c-3444-44c4-ad34-d1b7308230cc", "msg_id": "b21ab83f-8603-49dc-8f31-af1825c9305b"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "save_relationships_to_neo4j(result, driver)", "metadata": {"id": "263753f8-422d-435d-bd3b-6b6489bf8456", "msg_id": "aa262caf-7e66-472a-ac5e-d572de06e7f5"}, "outputs": [], "execution_count": null}]}